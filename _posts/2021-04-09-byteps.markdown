---
layout:     post
title:      "理解BytePS架构"
subtitle:   "BytePS Learning and Exploration"
date:       2021-04-09
author:     "Jiang Wenrui"
header-img: "img/post-bg-rwd.jpg"
tags:
    - 深度学习
    - 分布式
---

#### BytePS 初探

BytePS是一种对PS架构 和 All-reduce组合封装的分布式框架，主要采用途径为：
* [PSLite Push-Pull](https://github.com/dmlc/ps-lite): A light and efficient implementation of the parameter server framework
* [NCCL All-Reduce](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html#reducescatter): NCCL ReduceScatter and AllGather

其基础调度流程如下：
* **Computation**: Each GPU performs computation (forward/backward propagation), which is irrelevant to BytePS;
* **Local Reduce(NCCl ReduceScatter)**: Multiple GPUs on the same machine reduces the gradients;
* **Push**: The workers push the aggregated gradients to the servers;
* **Global Reduce**: Once the servers receive the gradients from different workers, it aggregates the gradients;
* **Pull**: The workers pull the aggregated gradients from the servers;
* **Local Broadcast(NCCl AllGather)**: The workers broadcasts the updated gradients to local GPUs;
* Goto next iteration and repeat from step 1.

#### BytePS 各类对象
LoopFunction集合： std::vector<LoopFunction>
> typedef void (*LoopFunction)(): LoopFunction是一个通用函数指针，指向一个不带参数、返回值为空的函数

* **PullLoop**
* **PcieReduceLoop**
* **PushLoop**
* **CoordinatePushLoop**
* **SyncNcclLoop**
* **CoordinateReduceLoop**
* **CoordinateBroadcastLoop**


QueueType集合：
* **COORDINATE_REDUCE**
* **REDUCE**
* **COPYD2H**
* **PCIE_REDUCE**
* **COORDINATE_PUSH**
* **COMPRESS**
* **PUSH**
* **PULL**
* **DECOMPRESS**
* **COPYH2D**
* **COORDINATE_BROADCAST**
* **BROADCAST**



#### BytePS 流程
分布式训练开启：
1. BytePSBasics.init
> 1. call operation.cc -> byteps::common::byteps_lazy_init
2. byteps::common::byteps_lazy_init
> 1. call BytePSGlobal::Init() -> BytePSGlobal::CreateScheduledQueue(QueueType)
> 2. 创建 LoopFunction 队列
> 3. call global.cc -> BytePSGlobal::Start
3. global.cc -> BytePSGlobal::Start
> 1. new std::thread(LoopFunction) 构造线程对象，可以开启线程执行以上LoopFunction

LoopFunction核心：
> * 定义在core_loops.cc
> * 根据QueueType call BytePSGlobal::GetScheduledQueue 获取 BytePSScheduledQueue
> * call scheduled_queue.cc -> BytePSScheduledQueue::getTask() task是一个std::shared_ptr<TensorTableEntry>
> * call core_loops.cc -> FinishOrProceed(task)

分布式训练循环：
1. Worker: compute gradients
2. DistributedGradientTape.push_pull_grads -> BytePSPushPullOp::ComputeAsync -> BytePSPushPullOp::StartTask
2. Worker: apply gradients

BytePSPushPullOp::StartTask做的事情即以下5个步骤：
1. **Local Reduce(NCCl ReduceScatter)**
2. **Push**
3. **Global Reduce**
4. **Pull**
5. **Local Broadcast(NCCl AllGather)**

BytePSPushPullOp::StartTask分析：
1. BytePSPushPullOp::StartTask
> 1. call common::GetPushQueueList
> 2. call common::GetPullQueueList
> 3. call operation.cc -> EnqueueTensor

2. common::GetPushQueueList
> 1. queue_list->push_back(REDUCE)
> 2. queue_list->push_back(COPYD2H)
> 3. queue_list->push_back(PCIE_REDUCE)
> 4. queue_list->push_back(PUSH)

3. common::GetPullQueueList 
> 1. queue_list->push_back(PULL)
> 2. queue_list->push_back(COPYH2D)
> 3. queue_list->push_back(BROADCAST)

4. operation.cc -> EnqueueTensor
> 1. call scheduled_queue.cc -> addTask(TensorTableEntry)



#### 参考资源

* [Byteps](https://github.com/bytedance/byteps): BytePS Official
* [新型分布式DNN训练架构-BytePS](https://coladrill.github.io/2020/12/19/%E6%96%B0%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8FDNN%E8%AE%AD%E7%BB%83%E6%9E%B6%E6%9E%84-BytePS/)


#### 转载声明

首次发布于 [Jiang Wenrui](http://wenruij.github.io)，转载请保留以上链接
