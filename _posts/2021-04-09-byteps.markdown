---
layout:     post
title:      "理解BytePS架构"
subtitle:   "BytePS Learning and Exploration"
date:       2021-04-09
author:     "Jiang Wenrui"
header-img: "img/post-bg-rwd.jpg"
tags:
    - 深度学习
    - 分布式
---

## BytePS 架构

BytePS是一种对PS架构 和 All-reduce组合封装的分布式框架，主要采用途径为：
* [PSLite Push-Pull](https://github.com/dmlc/ps-lite): A light and efficient implementation of the parameter server framework
* [NCCL All-Reduce](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html#reducescatter): NCCL ReduceScatter and AllGather

其基础调度流程如下：
* **Computation**: Each GPU performs computation (forward/backward propagation), which is irrelevant to BytePS;
* **Local Reduce(NCCl ReduceScatter)**: Multiple GPUs on the same machine reduces the gradients;
* **Push**: The workers push the aggregated gradients to the servers;
* **Global Reduce**: Once the servers receive the gradients from different workers, it aggregates the gradients;
* **Pull**: The workers pull the aggregated gradients from the servers;
* **Local Broadcast(NCCl AllGather)**: The workers broadcasts the updated gradients to local GPUs;
* Goto next iteration and repeat from step 1.

## BytePS 探索

#### BytePS 各类对象
LoopFunction集合： std::vector<LoopFunction>
> typedef void (*LoopFunction)(): LoopFunction是一个通用函数指针，指向一个不带参数、返回值为空的函数

* **PullLoop**
* **PcieReduceLoop**
* **PushLoop**
* **CoordinatePushLoop**
* **SyncNcclLoop**
* **CoordinateReduceLoop**
* **CoordinateBroadcastLoop**


QueueType集合：事件类型枚举 (共12种)
* **COORDINATE_REDUCE**
* **REDUCE**
* **COPYD2H**
* **PCIE_REDUCE**
* **COORDINATE_PUSH**
* **COMPRESS**
* **PUSH**
* **PULL**
* **DECOMPRESS**
* **COPYH2D**
* **COORDINATE_BROADCAST**
* **BROADCAST**

几个常见的对象：
* queue_list: std::vector(QueueType)
* TensorTableEntry: 即一个task, 里面封装着queue_list
* BytePSScheduledQueue:  里面封装着 std::vector<std::shared_ptr<TensorTableEntry>> _sq，同时一个BytePSScheduledQueue和一个QueueType一一对应


#### BytePS 流程
分布式训练开启：
1. BytePSBasics.init
> 1. call operation.cc -> byteps::common::byteps_lazy_init
2. byteps::common::byteps_lazy_init
> 1. call BytePSGlobal::Init() -> BytePSGlobal::CreateScheduledQueue(QueueType)，因为有12种QueueType，会创建12种BytePSScheduledQueue
> 2. 创建 LoopFunction 队列
> 3. call global.cc -> BytePSGlobal::Start
3. global.cc -> BytePSGlobal::Start
> 1. new std::thread(LoopFunction) 构造线程对象，可以开启线程执行以上LoopFunction

LoopFunction核心：
> * 定义在core_loops.cc
> * 根据QueueType call BytePSGlobal::GetScheduledQueue 获取 BytePSScheduledQueue
> * call scheduled_queue.cc -> BytePSScheduledQueue::getTask() 获取该QueueType对应的BytePSScheduledQueue中的task, task是一个std::shared_ptr(TensorTableEntry)
> * call core_loops.cc -> FinishOrProceed(task),FinishOrProceed做了两件事

1. 清除queue_list头部的QueueType： queue_list.erase(queue_list.begin())
2. 根据此时queue_list begin的QueueType get 对应BytePSScheduledQueue，然后将task封装入这个BytePSScheduledQueue

分布式训练循环：
1. Worker: compute gradients
2. DistributedGradientTape.push_pull_grads -> BytePSPushPullOp::ComputeAsync -> BytePSPushPullOp::StartTask
2. Worker: apply gradients

BytePSPushPullOp::StartTask做的事情即以下5个步骤：
1. **Local Reduce(NCCl ReduceScatter)**
2. **Push**
3. **Global Reduce**
4. **Pull**
5. **Local Broadcast(NCCl AllGather)**

BytePSPushPullOp::StartTask分析：
1. BytePSPushPullOp::StartTask
> 1. call common::GetPushQueueList
> 2. call common::GetPullQueueList
> 3. call operation.cc -> EnqueueTensor

2. common::GetPushQueueList
> 1. queue_list->push_back(REDUCE)
> 2. queue_list->push_back(COPYD2H)
> 3. queue_list->push_back(PCIE_REDUCE)
> 4. queue_list->push_back(PUSH)

3. common::GetPullQueueList 
> 1. queue_list->push_back(PULL)
> 2. queue_list->push_back(COPYH2D)
> 3. queue_list->push_back(BROADCAST)

4. operation.cc -> EnqueueTensor
> 1. call scheduled_queue.cc -> addTask(TensorTableEntry)

#### BytePS 数据承载
数据封装于 common.h -> struct BytePSContext 中

#### BytePS LoopFunction多线程同步 
训练开启后，StartTask首先创建queue_list：std::vector<QueueType>，顺序如下
> * COORDINATE_REDUCE/REDUCE
> * COPYD2H
> * PCIE_REDUCE
> * COMPRESS
> * PUSH
> * PULL
> * DECOMPRESS
> * COPYH2D
> * COORDINATE_BROADCAST/BROADCAST

StartTask call operation.cc -> EnqueueTensor, EnqueueTensor做几件事：
> 1. 创建task
> 2. 将queue_list封装入task: TensorTableEntry
> 3. add_task: 根据queue_list begin的QueueType get 对应BytePSScheduledQueue，然后将task封装入这个BytePSScheduledQueue

然后LoopFunction一直监听自己对应的BytePSScheduledQueue中是否有task，有就开始执行(也就是说LoopFunction的执行是被add_task触发)， 其循环执行以下：
1. get_task: 消费task
>  根据queue_list begin的QueueType get 对应BytePSScheduledQueue，并消费这个BytePSScheduledQueue中的task(数据就封装于task中)
2. run_task
3. add_task: 添加task(也就是queue_list中下一个事件task)
>  call FinishOrProceed(task)清除queue_list的begin，然后根据此时queue_list begin的QueueType get 对应BytePSScheduledQueue，并把task封装于其中

## 参考资源

* [Byteps](https://github.com/bytedance/byteps): BytePS Official
* [新型分布式DNN训练架构-BytePS](https://coladrill.github.io/2020/12/19/%E6%96%B0%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8FDNN%E8%AE%AD%E7%BB%83%E6%9E%B6%E6%9E%84-BytePS/)


## 转载声明

首次发布于 [Jiang Wenrui](http://wenruij.github.io)，转载请保留以上链接
