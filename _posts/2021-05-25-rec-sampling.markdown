---
layout:     post
title:      "个性化推荐-采样面面观"
subtitle:   "Sampling in Recommendation"
date:       2021-05-25
author:     "Jiang Wenrui"
header-img: "img/about-bg.jpg"
tags:
    - 深度学习
    - 个性化推荐
---

## 采样概述

推荐系统一般分为两部分，召回阶段和排序阶段。召回阶段是从全量数据中挑选出用户可能感兴趣的一部分数据，供后面的排序阶段使用.

## 召回采样
首先召回是一种集合选择方案。
召回是“是将用户可能喜欢的，和海量对用户根本不靠谱的，分隔开”，所以召回在线上所面对的数据环境，就是鱼龙混杂、良莠不齐。
所以，要求喂入召回模型的样本，既要让模型见过最匹配的，也要让模型见过最不靠谱的，才能让模型达到"开眼界、见世面"的目的，从而在“大是大非”上不犯错误。

所以召回的负样本优先采用 “全局负采样”， 通常我们采用概率采样，计算采样概率的概率函数。

但是，使用随机采样做负样本，也有其缺点，即与$d_+$相比，$d_-$与user太不匹配了。这样训练出来的模型，只能学到粗粒度上的差异，却无法感知到细微差别。就好比，一个推荐宠物的算法，能够正确做到向爱狗人士推荐狗，向爱猫人士推荐猫，但是在推荐狗的时候，无法精确感受到用户偏好上的细微差别，将各个犬种一视同仁地推出去。这样的推荐算法，用户也不会买账。

挖掘Hard Negative增强样本

## 排序采样
排序是一种排序预估方案，排序其目标是“从用户可能喜欢的当中挑选出用户最喜欢的”。排序是非常讲究所谓的“真负”样本的，即必须拿“曝光未点击”的样本做负样本。

## 粗排采样
粗排第一感可能会拿排序的采样方式。

第一版优化：正样本：真实曝光点击，负样本：召回输出中随机采样

第二版优化：正样本调整：直接学习精排模型的预估结果，将精排模型预估topn的结果作为粗排模型学习的正样本。
目的是使得粗排模型能够自适应地对齐精排模型目标的变化。
> 参考 [爱奇艺短视频推荐粗排篇](https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247533568&idx=1&sn=2b8c694ff43bb451ff1e70b49a7bd97b&chksm=fbd7f66ccca07f7a29d0eb0389cd4231707d6588219af0fff94b22e43d8106161172aff9f16c&scene=21#wechat_redirect): 级联模型

## 样本偏差
常见的样本偏差如：选择偏差，流行度偏差，位置偏差 etc
应对样本偏差的几种策略：
1. 数据扩充(选择偏差)
2. 二次采样(流行度偏差)
3. 通用框架(通用偏差)
4. 迁移学习ESAM(选择偏差)
5. 辅助学习ESMM(选择偏差)

下面依次看下这些方法：

#### 数据扩充
数据扩充事实上在图像CNN领域十分常见，比如训练数据中 图像旋转，裁剪都是一种扩充手段。

解决排序中的选择性偏差：

基于i2i图的多跳游走进行候选样本生成

首先看一下这种方式对建模方式的改变：
已有的CTR建模方法可以理解为u2i的建模，通常刻画了用户在特定请求上下文中对候选商品的偏好，而新的建模方式是去学习用户的每个历史点击商品和候选商品的关系

这种数据扩充的核心是：基于用户行为数据和商品多模态数据构建i2i图。

i2i图的构建，图中存在一种结点即商品结点，两种边关系即点击共现边和多模态向量边。
点击共现边通过用户的历史商品点击序列所构建，边的权重通过以下的公式得到，其在两个商品间的用户历史点击共现频数的基础上，考虑了每次点击共现的时间间隔因子。
形成样本时最多会做二跳。几种不同的i2i关系包括：
1. 基于点击一跳邻居构建i2i
2. 基于向量一跳邻居构建i2i
3. 基于点击-点击二跳游走构建i2
4. 基于点击-向量二跳游走构建i2
5. 基于向量-点击二跳游走构建i2i

> 参考：[Debiasing比赛冠军技术方案及在美团的实践](https://tech.meituan.com/2020/08/20/kdd-cup-debiasing-practice.html)

#### 二次采样

也就是我们常说的 热度打压，参考：[二次采样](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter10_natural-language-processing/10.3_word2vec-pytorch?id=_10312-%e4%ba%8c%e6%ac%a1%e9%87%87%e6%a0%b7)

#### 通用框架

系统通过引入去偏参数，使得偏差解决具备普适性，去解决通用偏差问题。
其将偏差问题的本质描述为：

> 训练数据往往是观测所得, 训练数据的概率分布和预测数据的概率分布存在不一致，这种不一致将导致根据训练数据学习出的模型与真实最优模型之间的差异

因而提出 重加权(re-weighting)的方法来做去偏--对每个训练集中的样本加上特定的权重. 
通过引入参数 使得训练数据的概率分布和预测数据的概率分布 之间的差异能够得到捕捉。

#### 迁移学习

用于解决召回中的选择性偏差.

首先基于 曝光样本训练base model。
因为只有曝光item才有label，所以只能在曝光item上训练“item映射函数”Func(expose)，导致学习出来的Func(expose)会将“曝光item”和“未曝光item”映射到向量空间中相距较远的两个区域，使训练出来的召回模型在面对大量“未曝光item”时失效

即曝光数据 和 未曝光数据 分布上差异 造成了在“曝光item”上训练出来的召回模型，上线后，作用于大量的“未曝光数据”后，表现得一塌糊涂。

作者认为 “曝光item”与“未曝光item”的特征之间的关系是一致的，通过这种一致性关系约束，
可以将"曝光item"与“未曝光item”映射到同一片向量区域。

这种约束，发生在原始特征之间，ESAM认为经过Func(expose)的映射，“曝光item”与“未曝光item”的高阶特征(embedding的不同维度)之间的关系，也应该保持不变。

如下图所示：
<img src="/img/sampling/ESAM_01.png" width="80%" height="80%" />

具体的约束方式为 再原有的Loss上添加 Loss(DA):
1. 收集了n个“曝光item”(来自source domain)和用户对它们的反馈。这n个item经过Func(expose)映射，映射成Ds，L是embedding的维度
2. 再随机抽样n个“未曝光item”(来自target domain)，它们经过Func(expose)映射，映射成Dt
3. 要求Ds的L维高阶特征的两两之间(h(s_j)和h(s_k))的协方差，与Dt的L维高阶特征的两两之间(h(d_j)和h(d_k))的协方差，两者之间的差值尽可能小

如下图所示：
<img src="/img/sampling/ESAM_02.png" width="80%" height="80%" />

#### 辅助学习

## 参考资源

* [Debiasing比赛冠军技术方案及在美团的实践](https://tech.meituan.com/2020/08/20/kdd-cup-debiasing-practice.html)
* [阿里CVR预估模型之ESMM](https://zhuanlan.zhihu.com/p/57481330)
* [负样本为王](https://zhuanlan.zhihu.com/p/165064102)
* [阿里ESAM:用迁移学习解决召回中的样本偏差](https://zhuanlan.zhihu.com/p/335626180)
* [AutoDebias](https://arxiv.org/pdf/2105.04170v1.pdf)
* [AutoDebias解读CSDN](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/117308591)
* [AutoDebias解读知乎](https://zhuanlan.zhihu.com/p/375980125)
* [负样本概率采样](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter10_natural-language-processing/10.3_word2vec-pytorch?id=_1032-%e8%b4%9f%e9%87%87%e6%a0%b7)

## 转载声明

首次发布于 [Jiang Wenrui](http://wenruij.github.io)，转载请保留以上链接